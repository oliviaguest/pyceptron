\documentclass{beamer}
% \usefonttheme{professionalfonts}
\usefonttheme{professionalfonts}

\usepackage{amsmath}
\usepackage{cancel}
%%%%%%%%%%%%%%%%%%%%%%%%% For making hand-outs %%%%%%%%%%%%%%%%%%%%%%%%%%%
% \documentclass[handout]{beamer}
% \usepackage{pgfpages}
% \pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{inputred}{HTML}{c30e0e}
\definecolor{hiddenblue}{HTML}{2626c9}
\definecolor{outputgreen}{HTML}{008000}


% \usepackage{eulervm}
\usepackage{default}
\usepackage{caption}
\usepackage{booktabs,mathptmx,siunitx}
% \usefonttheme{serif}
\graphicspath{{img/}}
\captionsetup{font=scriptsize, labelfont=scriptsize}
\begin{document}


\begin{frame}
% % \frametitle{What is programming?}
% \framesubtitle{It is like use using any other language!}
% 
% ``Getting an education was a bit like a communicable sexual disease. It made you unsuitable for a lot of jobs and then you had the urge to pass it on.''
% \\ \
\centering\Huge Welcome to the computational cognitive modelling workshop! 
\vfill \huge
\centering\textbf{Part 2: Artificial neural networks} \normalsize
\vfill
% \textit{Olivia Guest \hfill  Chris Brand \\  \ Nick Sexton \hfill Nicole Cruz De Echeverria Loebell } 
% -- \textit{Terry Pratchett}
\end{frame}

\begin{frame}
% % \frametitle{What is programming?}
% \framesubtitle{It is like use using any other language!}
% 
% ``Getting an education was a bit like a communicable sexual disease. It made you unsuitable for a lot of jobs and then you had the urge to pass it on.''
% \\ \
\vfill \Huge
\centering Part 2: \textbf{Artificial neural networks} \large
\vfill
\textit{
Olivia Guest \hfill  Chris Brand 
\vspace{0.5cm} \\ \ 
Nick Sexton \hfill Nicole Cruz De Echeverria Loebell } 
% -- \textit{Terry Pratchett}
\end{frame}


\begin{frame}
\frametitle{What is a neural network?}
\framesubtitle{A mathematical model}
 \begin{columns}[T]
    \begin{column}{.5\textwidth} 
     \ \\ 
     \ \\
\begin{itemize}
\item Inspired by the nervous system \\ \
 \item A set of \emph{units}, connected by \emph{weights} \\ \
\item The network \emph{runs} by passing \emph{activations} from the \emph{input} (to the \emph{hidden}) to the \emph{output} units \\ \
\end{itemize}
\end{column}
\begin{column}{.5\textwidth}
\begin{figure}
 
 \includegraphics[width=\linewidth]{./fig/3-layer.pdf}
 % 3-layer.pdf: 236x271 pixel, 72dpi, 8.33x9.56 cm, bb=0 0 236 271
 \caption{Glosser.ca / CC-BY-SA-3.0}
\end{figure}
\end{column}

\end{columns}
\end{frame}




\begin{frame}
\frametitle{Why use artificial neural networks for modelling?}
\framesubtitle{Some aspects of their behaviour are like their namesake!}
\begin{itemize}
\item Learn pretty much any input-output data \\ \
 \item Uncover rules on their own about data  \\ \
\item Generalise from what they have learnt \\ \
\item Cope with noise and damage \\ \
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{How does an artificial neural network run?}
\framesubtitle{By using maths, predictably!}
  \begin{columns}[T]
    \begin{column}{.5\textwidth}
    \begin{enumerate}
%      \begin{block}{Run network}
\vfill
\item<1->{\textcolor{inputred}{Input units} are set to a \emph{pattern}} \\ \ \\
\item<3->{Calculate \textcolor{hiddenblue}{hidden units}' states}\only<5-9>{: \\ \ \\
\begin{tabular}{r S[tabformat=3.2] r}% syntax for siunitx v2; for v1 use "tabformat"
$1 \times 0.5~ =$ & $0.5$ &\\
\only<6->{ $1 \times 0.0 ~=$ & $0.0$ &\\}
 \only<7>{$0 \times 0.8 ~=$ & $0.0$ & \\}
  \only<8->{$0 \times 0.8 ~=$ & $0.0$ & +\\
\hline
 & $0.5$ &}
\end{tabular}


}

\ \\ 
\item<11->{Same for \textcolor{outputgreen}{output units}}\only<13-18>{: \\
\ \\
\begin{tabular}{r S[tabformat=3.2] r}% syntax for siunitx v2; for v1 use "tabformat"
 \only<13-18>{$0.5 \times 0.25~ =$ & $0.125$ &\\}
\only<14-18>{ $0.3 \times 1.5  ~=$ & $0.45$ &\\}
\only<15-18>{ $1.6 \times -0.3 ~=$ & $-0.48$ &\\}
 \only<16-18>{$-0.4 \times 1.1 ~=$ & $-0.44$ &  \\}
 \only<17-18>{$-0.4 \times 1.1 ~=$ & $-0.44$ & + \\
 \hline
 & $-0.345$ & }
\end{tabular}}
\end{enumerate}
% \end{block}
    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
% \begin{figure}
\includegraphics<1>[width=\linewidth]{./fig/3-layer.pdf}
\includegraphics<2-3>[width=\linewidth]{./fig/3-layer_propagate_1.pdf}
\includegraphics<4-8>[width=\linewidth]{./fig/3-layer_propagate_2.pdf}
\includegraphics<9>[width=\linewidth]{./fig/3-layer_propagate_3.pdf}
\includegraphics<10-11>[width=\linewidth]{./fig/3-layer_propagate_4.pdf}
\includegraphics<12-17>[width=\linewidth]{./fig/3-layer_propagate_5.pdf}
\includegraphics<18-19>[width=\linewidth]{./fig/3-layer_propagate_6.pdf}
% \includegraphics<11->[width=\linewidth]{./fig/3-layer_propagate_5.pdf}
%  \only<6>{\includegraphics[width=\linewidth]{./fig/3-layer_propagate_4.pdf}}
%   \caption{Glosser.ca / CC-BY-SA-3.0}
% \end{figure}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}



\begin{frame}
\frametitle{How does an artificial neural network run?}
\framesubtitle{By using maths, predictably!}
  \begin{columns}[T]
    \begin{column}{.5\textwidth}
     \ \\
     \ \\
     
But we/programmers are lazy\only<2->{:
\only<2-6>{\\ \ \\ we like to give things general names}

\only<7->{
    \begin{overprint}
        \onslide<7-9>
	\begin{equation*}
	a_i = \onslide<11->{f\left(} \onslide<9>{ \sum^{\textcolor{white}{N}}}   \onslide<8->{x_j \times w_{ji}}  \onslide<11->{\right)}
	\end{equation*}
	\onslide<10->
	\begin{equation*}
	a_i = \onslide<11->{f\left(}  \sum_{j=1}^N  x_j \times w_{ji}  \onslide<11->{\right)}
	\end{equation*}
    \end{overprint}
} }
    
\only<12->{where $a_i$ is the unit whose state we want to calculate, $N$ is the number of units on the previous layer, $w_{ji}$ is the weight on the connection between $i$ and $j$, and $f$ is a function, commonly the logistic step function.}

% Which means, to calculate, e.g., $a_1$ multiply all the states in layer before with the incoming weight and then add them up. So in our case all the $x_1~...~x_3$ because $N = 3$.  
% \end{block}
    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
% 	\begin{figure}


	\includegraphics<1>[width=\linewidth]{./fig/3-layer.pdf}
	\includegraphics<2>[width=\linewidth]{./fig/3-layer.pdf}
	\includegraphics<3>[width=\linewidth]{./fig/3-layer_maths_1.pdf}
	\includegraphics<4>[width=\linewidth]{./fig/3-layer_maths_2.pdf}
	\includegraphics<5>[width=\linewidth]{./fig/3-layer_maths_3.pdf}
	\includegraphics<6->[width=\linewidth]{./fig/3-layer_maths.pdf}
% 	\includegraphics<6>[width=\linewidth]{./fig/3-layer_maths.pdf}
% 	\includegraphics[width=\linewidth]{./fig/3-layer_maths.pdf}
% 	\includegraphics[width=\linewidth]{./fig/3-layer_maths.pdf}
% 	  \caption{Glosser.ca / CC-BY-SA-3.0}
% 	\end{figure}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
\frametitle{How do networks learn?}
\framesubtitle{Cunning!}
\begin{itemize}
\item<1-> Many options: Hebbian learning, back-propagation of error, Boltzmann machine learning, self-organising map algorithm, etc. \\ \

\item<2-> All learning algorithms work by changing the connection weights \\ \

\item<3-> Learning can be divided into \emph{supervised}, \emph{unsupervised}, and \emph{reinforcement} \\ \ \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hebbian learning}
\framesubtitle{A very simple learning rule}
  \begin{columns}[T]
    \begin{column}{.5\textwidth}
    \ \\ 
%     \onslide<1->{
%     Units that are \emph{on} together in the pattern set are made to \emph{be} on together when the network is run:

     \onslide<1->{\small ``Cells that fire together, wire together'' \\ \
     --- Carla Shatz }
      
%       \begin{overprint}
        \onslide<2->{
	\begin{equation*}
	\onslide<4->{\Delta} w_{ij} = \onslide<3->{\eta \times}   x_{i}   \times a_j
	\end{equation*}}
% 	\onslide<3->
% 	\begin{equation*}
% 	\onslide<5->{\Delta} w_{ij} = \onslide<4->{\eta} \sum^P_{p=1}    x_{pi}  \times a_j
% 	\end{equation*}
%     \end{overprint}

      
      
      \ \\
      
      \onslide<5->{which means each weight, $w_{ij}$ is changed by a small in/decrement for every pattern}

%        \onslide<6-> {If $x_i$ or $a_j$ is on, then the other will also be on}
             

    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
 
 \includegraphics<1->[width=\linewidth]{./fig/3-layer_maths.pdf}
%   \caption{Glosser.ca / CC-BY-SA-3.0}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
\frametitle{Hebbian learning}
\framesubtitle{``Cells that fire together, wire together'' --- Carla Shatz }
  \begin{columns}[T]
    \begin{column}{.5\textwidth}
    \ \\ 
    \only<1->{\only<18>{\bfseries} Hebb's rule is simple, but \emph{very unstable}!}
      \ \\
      \mdseries
%     Units that are \emph{on} together in the pattern set are made to \emph{be} on together when the network is run:
	\only<2->{\begin{equation*}
	\Delta w_{ij} = \eta  \times x_i  \times  a_j
	\end{equation*}}    
	\ \\	
	\ \\	
     \begin{overprint}
        \only<3>{
	\begin{equation*}
	\Delta w_{11} = \eta \times  x_i  \times a_j
	\end{equation*}}
	\only<4>{
	\begin{equation*}
	\Delta w_{11} = 0.5 \times  x_i  \times a_j
	\end{equation*}}
	\only<5>{
	\begin{equation*}
	\Delta w_{11} = 0.5 \times  x_1  \times a_j
	\end{equation*}}
	\only<6>{
	\begin{equation*}
	\Delta w_{11} = 0.5 \times  x_1  \times a_1
	\end{equation*}}
	\only<7>{
	\begin{equation*}
	\Delta w_{11} = 0.5 \times  1.0  \times a_1
	\end{equation*}}
	\only<8>{
	\begin{equation*}
	\Delta w_{11} = 0.5 \times  1.0  \times 0.3
	\end{equation*}}
	\only<9-17>{
	\begin{equation*}
	\Delta w_{11} = 0.15
	\end{equation*}}
    \end{overprint}
    
    \begin{center}
     

        \begin{overprint}
    	\only<10>{
%     	\begin{equation*}
	$\mathbf{new}~w_{11} = \mathbf{old}~ w_{11} + \Delta w_{11}$
% 	\end{equation*}
	}
    	\only<11>{
%     	\begin{equation*}
	$\mathbf{new}~w_{11} = 0.0 + \Delta w_{11}$
% 	\end{equation*}
	  }
    	\only<12>{
%     	\begin{equation*}
	$\mathbf{new}~w_{11} = 0.0 + 0.15$
% 	\end{equation*}}
	}
	\only<13>{
%     	\begin{equation*}
	$\mathbf{new}~w_{11} = 0.15$
% 	\end{equation*}
	  }
	  \only<14>{
%     	\begin{equation*}
	$w_{11} = 0.15$
% 	\end{equation*}
	  }
	  \only<15>{
%     	\begin{equation*}
	$w_{11} = 0.15$ + something positive
% 	\end{equation*}
	  }	\only<16>{
%     	\begin{equation*}
	$w_{11} = 0.15$ + something positive + something else positive +
% 	\end{equation*}
	  }	\only<17>{
%     	\begin{equation*}
	$w_{11} = 0.15$ + something positive + something else positive + another positive value + ...
% 	\end{equation*}
	  }
    \end{overprint}    
    \end{center}

% 	\begin{equation*}
% 	\Delta w_{11} = 0.5 \times  x_1  \times a_1
% 	\end{equation*}   
% 	
% 	\begin{equation*}
% 	\Delta w_{11} = 0.5 \times  1.0  \times a_1
% 	\end{equation*}  
% 	
% 	\begin{equation*}
% 	\Delta w_{11} = 0.5 \times  1.0  \times 0.3
% 	\end{equation*}  
% 	$\Delta w_{11} = x_1 \times  a_1 $
     
    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
\begin{figure}
 
 \includegraphics<1->[width=\linewidth]{./fig/3-layer_maths.pdf}
%   \caption{Glosser.ca / CC-BY-SA-3.0}
\end{figure}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}
\frametitle{The perceptron}
\framesubtitle{A simple classifier}
  \begin{columns}[T]
    \begin{column}{.5\textwidth}    \ \\ \ \\ 
    \ \\
    \begin{itemize}
    
 
     \item<2->Created in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt  \\ \
\item<3->Linear classifier \\ \

\item<4->Simplest form of feedforward network \\ \

% \item Consists of two layers of units \\ \
% \item No hidden units \\ \
\end{itemize}

    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
\begin{figure}
 
 \includegraphics[width=0.9\linewidth]{./fig/perceptron.pdf}
%   \caption{Glosser.ca / CC-BY-SA-3.0}
\end{figure}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}
\frametitle{How does the perceptron learn?}
\framesubtitle{Maths again!}
  \begin{columns}[T]
    \begin{column}{.5\textwidth} 
    \ \\
    \ \\
\begin{enumerate}

 \item<1-> Initialise weights
 \\ \
 \item<2-> Run network \onslide<2>{using: 
 
 \begin{equation*}
y_j = f \left( \sum_1^N w_i \times x_i \right)
 \end{equation*} 
 \\ \
 
same as always!
\\ \
}

% \item<3-> Update weights using: 
%  
%  \begin{equation*}
%   \Delta w_i = \eta ( d_j - y_j ) \times x_{i}
%  \end{equation*} 
%   \ \\
%  
%  where $d$ is what we want $y$ to be given $x_i$, and $\eta$ is the learning rate.

 \end{enumerate}

    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
\begin{figure}
 
 \includegraphics<1->[width=0.9\linewidth]{./fig/perceptron_maths.pdf}
%   \caption{Glosser.ca / CC-BY-SA-3.0}
\end{figure}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}
\frametitle{How does the perceptron learn?}
\framesubtitle{Maths again!}
  \begin{columns}[T]
    \begin{column}{.5\textwidth} 
        \  \\
 \   \\ 
\begin{enumerate}
 \item Initialise weights
 
 \ \\
 \item Run network
 
 \ \\
 
 \item Update weights using:

\end{enumerate}
 
 \begin{equation*}
  \Delta w_i = \eta ( d_j - y_j ) \times x_{i}
 \end{equation*} 
  \ \\
 
 where $d$ is what we want $y$ to be given $x_i$, and $\eta$ is the learning rate.

    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
\begin{figure}
 
 \includegraphics[width=0.9\linewidth]{./fig/perceptron_maths.pdf}
%   \caption{Glosser.ca / CC-BY-SA-3.0}
\end{figure}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}
\frametitle{How does the perceptron learn?}
\framesubtitle{Maths again!}
  \begin{columns}[T]
    \begin{column}{.5\textwidth} 
        \  \\
 \   \\ 
\begin{enumerate}
 \item<1-> Initialise weights
 
 \ \\
 \item<1-> Run network
 
 \ \\
 
 \item<1-> Update weights
 
  \ \\
 
 \item<1-> Repeat 2 and 3
 
 \ \\

 \item<2> When do we stop?
\end{enumerate}


    \end{column}
    \begin{column}{.5\textwidth}
%     \begin{block}{}
% Your image included here
\begin{figure}
 
 \includegraphics<1->[width=0.9\linewidth]{./fig/perceptron_maths.pdf}
%   \caption{Glosser.ca / CC-BY-SA-3.0}
\end{figure}
%     \end{block}
    \end{column}
  \end{columns}
\end{frame}




\begin{frame}
\frametitle{The end}
\framesubtitle{}
        \  \\
 \   \\ 

 \Huge Time to program a perceptron!
\end{frame}




\end{document}
